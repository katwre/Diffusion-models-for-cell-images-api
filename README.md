# Introduction

<figure>
<p align="center">
  <img src="img/merged.gif" width="720">
</p>
  <figcaption align="center"><b>Figure.</b> Interactive diffusion-based inpainting results on an H&E image of a monocyte, illustrating reconstruction of masked regions while preserving the observed context.</figcaption>
</figure>
</br></br>

-----------------------------

The aim of this project is to explore the ability of diffusion models to generate missing regions in images. It restores corrupted areas in H&E cell images using a compact diffusion model and a simple Streamlit UI. 

Recognizing blood cell types in microscopic images is central to clinical diagnostics. While the task has been studied for decades, recent machine learning and deep learning methods continue to improve accuracy ([Acevedo et al., 2019](https://www.sciencedirect.com/science/article/abs/pii/S0169260719303578?via%3Dihub); [Deltadahl et al., 2025](https://www.nature.com/articles/s42256-025-01122-7)). Rather than classifying immune cells, this project applies diffusion-based inpainting‚Äîwith a linear noise schedule and standard Œµ-prediction‚Äîto restore user-marked regions of H&E images. The training data comprises 17,092 expert-annotated single-cell H&E images (JPG, 360√ó363 px) captured with the CellaVision DM96 and published by [Acevedo et al., 2019](https://www.sciencedirect.com/science/article/abs/pii/S0169260719303578?via%3Dihub) (URL: https://data.mendeley.com/datasets/snkd93bnjr/1). Classes include neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes, erythroblasts, and platelets. H&E staining combines hematoxylin (nuclei, purplish blue) and eosin (cytoplasm/extracellular matrix, pink).


If you're interested in other diffusion-based generative methods on H&E images, check out a model called [CytoDiffusion](https://www.nature.com/articles/s42256-025-01122-7) by Deltadahl et al, Nature Machine Intelligence 2025. It's is a newly published diffusion-based model designed specifically for blood cell morphology analysis. Instead of being a traditional classifier that only learns boundaries between labeled categories, it models the full distribution of blood cell appearances using generative diffusion techniques.

# Tech Stack

### Machine Learning | Data Science

üß™ Diffusion model ‚Ä¢ UNet ‚Ä¢ linear noise schedule  
üß† NumPy ‚Ä¢ Pandas ‚Ä¢ scikit-learn  
üî• PyTorch  
üìä Matplotlib ‚Ä¢ Seaborn  


### MLOps & Deployment

üé® Streamlit  
üê≥ Docker  
‚òÅÔ∏è AWS Batch (on-demand GPU jobs for diffusion model sampling/inference)  
üì¶ AWS ECR


# Methods

## Features

- Interactive inpainting in the browser (Streamlit)
- Paint masks to define regions to restore
- Checkpointed UNet-based DDPM (64√ó64) for fast sampling
- Composite the model‚Äôs prediction back onto the original image size
- Run locally, via Docker or in the cloud via AWS EC2


## Project Structure

### Phase 1 - Diffusion 101

Here, labels of the cell types are ignored and the model is trained in an unconditional setting.

- Images are resized to 64 √ó 64 pixels.
- A small DDPM is trained on single-cell images.
- The goal is to generate plausible single-cell images without any conditioning.

This phase is designed to introduce and solidify the core concepts of diffusion models, including:

- The forward and reverse diffusion processes  
- Noise schedules  
- U-Net conditioning on the diffusion timestep

Results analysis can be found in `notebooks/01_unconditional_diffusion.ipynb`.

<figure>
<p align="center">
  <img src="img/phase1.png" width="400" />
</p>
  <figcaption align="center"><b>Figure.</b> Unconditional samples generated by the diffusion model after training on single-cell H&E images.</figcaption>
</figure>
</br></br>

---

### Phase 2 - Inpainting Diffusion

This phase demonstrates the model's ability to reason about missing visual information and to generate plausible cell morphology under partial observation (image inpainting).

- Input images are corrupted using a binary mask.
- The model input is constructed by concatenating:
  - the masked image, and
  - the corresponding mask.
- The model is trained using standard Œµ-prediction.
- During sampling, known pixels are clamped to enforce consistency with the observed image.

Results analysis can be found in `notebooks/02_Inpainting_Diffusion.ipynb`.

<figure>
<p align="center">
  <img src="img/phase2.png" width="500" />
</p>
  <figcaption align="center"><b>Figure.</b> Diffusion-based inpainting results. The model reconstructs masked regions while preserving the observed context.</figcaption>
</figure>
</br></br>

---

### Future extensions

Possible extensions include:

- Conditioning the diffusion model on cell-type labels using learned embeddings
- Generating samples under partial observation, e.g.  
  *"Generate an erythroblast image with the nucleus region masked."*
- Evaluating how class conditioning influences structural and morphological fidelity

----

# Deployment

Demo inference: upload an image + mask -> run inpainting sampling -> show result

My steps:

- a Streamlit UI that accepts inputs, preprocesses them exactly like training, and
- an inference function that loads the trained inpainting DDPM and runs clamped sampling with the mask


#### Locally

Run locally:
- Pipenv
```bash
  pipenv install
  pipenv run streamlit run app/app.py
```
- via Docker (build and run from the project root)
```bash
docker build -t diffusion-inpaint .
docker run --rm -p 8501:8501 diffusion-inpaint
# Open http://localhost:8501 in your browser
```


### AWS Batch

First push images to ECR. `Dockerfile` corresponds to UI container (Streamlit) for ECS Fargate (CPU, always on). `Dockerfile.gpu` corresponds to a worker container for AWS Batch (GPU, runs per inference job). Communication between them happens through AWS APIs + S3, not direct container-to-container calls.The reason behind it to make UI stay cheap/always-on (Fargate CPU), and GPU spins up only per request (Batch), so no idle GPU cost. Fargate is just the serverless compute engine that can run tasks for ECS or EKS.

```bash
docker build -f Dockerfile -t diffusion-inpaint-ui .
docker build -f Dockerfile.gpu -t diffusion-inpaint-gpu .
```
A quick test of the UI image:
```bash
docker run --rm -p 8501:8501 diffusion-inpaint-ui
```

Push images to a ECR repo:
```bash
# Set vars:
AWS_REGION=eu-west-1
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
GPU_REPO=diffusion-inpaint-gpu
UI_REPO=diffusion-inpaint-ui

# Create repos
aws ecr create-repository --repository-name "$GPU_REPO" --region "$AWS_REGION"
aws ecr create-repository --repository-name "$UI_REPO" --region "$AWS_REGION"

# Login Docker to ECR:
aws ecr get-login-password --region "$AWS_REGION" | docker login --username AWS --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

# Tag local images:
docker tag diffusion-inpaint-gpu:latest "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$GPU_REPO:latest"
docker tag diffusion-inpaint-ui:latest "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$UI_REPO:latest"

# Push
docker push "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$GPU_REPO:latest"
docker push "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$UI_REPO:latest"

# Verify
aws ecr describe-images --repository-name "$GPU_REPO" --region "$AWS_REGION"
aws ecr describe-images --repository-name "$UI_REPO" --region "$AWS_REGION"

```


Create S3 bucket:
```bash
BUCKET_NAME="diffusion-inpaint-$(date +%s)-eu-west-1"
aws s3 mb "s3://$BUCKET_NAME" --region "$AWS_REGION"
echo "Bucket: $BUCKET_NAME"
```


Create AWS Batch (EC2 GPU compute env, g4dn.xlarge, min vCPU 0) + queue + job definition using diffusion-gpu.
```bash
Go to IAM -> Roles -> Create role
Trust entity: AWS service ‚Üí Batch
Name: BatchServiceRole
Add policy: AWSBatchServiceRolePolicy (auto-attached)
Create.
Repeat for EC2 instance role:
Trust entity: EC2
Name: BatchEC2InstanceRole
Add policies: AmazonEC2ContainerServiceforEC2Role + custom S3 policy:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject", "s3:PutObject"],
      "Resource": "arn:aws:s3:::BUCKET_NAME/*"
    }
  ]
}
```

Create Compute Environment:
```bash
Open AWS Console ‚Üí AWS Batch ‚Üí Compute environments
Click Create
Type: Managed
Compute environment type: EC2
Name: diffusion-gpu-ce
Instance types: add g4dn.xlarge
Allocation strategy: Best fit (default is fine)
Minimum vCPUs: 0
Desired vCPUs: 0
Maximum vCPUs: 4
EC2 key pair: None (unless you want SSH)
Instance role: BatchEC2InstanceRole (the EC2 role you created)
Service role: AWSBatchServiceRole
VPC/Subnets/Security group: pick defaults (public subnets are fine)
Click Create compute environment
```

Create Job Queue:
```bash
AWS Batch ‚Üí Job queues
Click Create
Name: diffusion-gpu-queue
Priority: 1
State: Enabled
Compute environments: select diffusion-gpu-ce
Name: inpaint-gpu-queue
Click Create job queue
```

Create Job Definition:
```bash
AWS Batch ‚Üí Job definitions
Click Create
Name: diffusion-gpu-job
Type: Container
Container image:
<AWS_ACCOUNT_ID>.dkr.ecr.eu-west-1.amazonaws.com/diffusion-inpaint-gpu:latest
vCPUs: 4
Memory: 16000 (MB)
GPUs: 1
Command:
python /src/diffusion_cells/infer_batch.py
Job role: (optional) a role with S3 Get/Put for your bucket
Click Create job definition
```
